{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rough-electron",
   "metadata": {},
   "source": [
    "# Solving-Hamilton-Jacobi-Bellman Equations (via FBSDEs)\n",
    "#### Frederik Kelbel, Imperial College London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from operators import div, Î”, D, mdotb, bdotm, mdotm, bdotb\n",
    "from FBSDEs import FBSDESolver\n",
    "from pdes import FBSDE\n",
    "from plotly.subplots import make_subplots\n",
    "from configs import CONFIG_FBSDES as MODEL_CONFIG\n",
    "from itertools import product\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-duncan",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses, avg_over=10):\n",
    "    avgs = np.convolve(losses, np.ones(avg_over), 'valid') / avg_over\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(avgs)), y=avgs, mode='lines', name=\"Error at x=0.1\"), row=1, col=1)\n",
    "    fig.update_layout(\n",
    "        title=\"Loss\",\n",
    "        xaxis_title=\"Iterations\",\n",
    "        yaxis_title=\"Loss\",\n",
    "        font=dict(\n",
    "            family=\"Courier New, monospace\",\n",
    "            size=14\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-recipe",
   "metadata": {},
   "source": [
    "### The Merton Problem (Wealth Allocation Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-curtis",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "dX_s = ((\\mu -r)u_s + r)X_s ds + \\sigma u_s X_s dW_s, \\; s \\in [0, T] \\\\\n",
    "X_0 = x > 0\n",
    "\\end{cases},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-semester",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "\\partial_t J(t, x) + \\sup_{u} \\Big\\{ ((\\mu-r)u + r)x \\partial_x J(t, x) + \\frac{1}{2} \\sigma^2 u^2 x^2\\partial_{xx} J(t, x) \\Big\\} = 0 \\text{ on $[0, T] \\times (0, \\infty)$}\n",
    "\\\\\n",
    "J(T, x) = x^\\gamma \\text{ $\\forall x > 0$}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-participant",
   "metadata": {},
   "source": [
    "Minimum at $u^* = \\frac{(r-\\mu)J_x}{\\sigma^2 x J_{xx}}$. Equation becomes\n",
    "$J_t + r x J_x - \\frac{1}{2} \\frac{(r-\\mu)^2J^2_x}{J_{xx}} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "LQR_MODEL_CONFIG = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_discretisation_steps\": 30,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"network_type\": \"MINI\",\n",
    "    \"optimiser\": \"Adam\"\n",
    "}\n",
    "model = LQR_MODEL_CONFIG\n",
    "class LQR(FBSDE):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        sigma = 0.3*torch.ones((model[\"batch_size\"], 1, 1))\n",
    "        self.h = lambda X, Y, Z, t: C(X) + (1/2)*torch.einsum(\"bi, bij, bj -> b\", Z, M, torch.einsum(\"bij, bj -> bi\", (-inv_D @ M), Z)).unsqueeze(1)\n",
    "        \n",
    "        self.b = lambda X, t: 0.1*X\n",
    "        self.sigma = lambda X, t: sigma\n",
    "        \n",
    "        self.terminal_condition = lambda X: 0.001*X**2\n",
    "        \n",
    "        self.var_dim = 1\n",
    "        self.terminal_time = 1     \n",
    "        self.init_sampling_func = lambda X: (X-0.5)*2\n",
    "        self.control_noise = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-rehabilitation",
   "metadata": {},
   "source": [
    "## This would require 2FBSDE system. Meaning we would have to compute the Hessian for every batch entry and more (see papers). What if we did determine the optimal control function and do a policy iteration over the control as they do in PIADGM, so transform into FBSDE taking inf every iteration, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-reward",
   "metadata": {},
   "source": [
    "This\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\partial_t J(t, x) + \\inf_{u} \\Big\\{ \\frac{1}{2} tr(\\mathcal{H_J \\sigma \\sigma^T}) + [H x + M u] \\nabla_x^T J(t, x) + C(x) + \\frac{1}{2} u^T D u \\Big\\} = 0 \\text{ on $[0, T] \\times (-\\infty, \\infty)$}\n",
    "\\\\\n",
    "J(T, x) = Rx^2 \\text{ $\\forall x \\in \\mathbb{R}$}\n",
    "\\end{cases}\n",
    "$$ becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-healthcare",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    &\\begin{cases}\n",
    "        dX_t = [H(t, X_t)+Mu] dt + \\sigma(t, X_t) dW_t, \\quad t \\in [0, T] \\\\\n",
    "        X_0 = x\n",
    "    \\end{cases}, \\\\\n",
    "    &\\begin{cases}\n",
    "        dY_t = C(X_t) dt + \\frac{1}{2} D u^2 dt + \\nabla J^{* \\; T}(t, X_t) \\sigma(t, X_t)  dW_t, \\quad t \\in [0, T] \\\\\n",
    "        Y_T = g(X_T)\n",
    "    \\end{cases}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-glenn",
   "metadata": {},
   "source": [
    "And take $u_\\theta$ such that the loss $\\mathcal{L}(u) = (M u) (\\nabla J^*)^T + \\frac{1}{2} u^T D u$ is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-bosnia",
   "metadata": {},
   "source": [
    "### Linear-quadratic control problem 1-dimensional (Riccati Equation) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-balance",
   "metadata": {},
   "source": [
    "Let $(\\Omega, \\mathcal{F}, \\{\\mathcal{F}_t\\}_{t\\in [0, T]}, \\mathbb{P})$. We consider\n",
    "$$\n",
    "\\begin{cases}\n",
    "dX_s = [H_s(X_s) + M_s(X) u_s] ds + \\sigma_s dW_s, \\; s \\in [0, T] \\\\\n",
    "X_0 = x > 0\n",
    "\\end{cases},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-individual",
   "metadata": {},
   "source": [
    "We aim to maximise\n",
    "$$\n",
    "J^u(t, x) := \\mathbb{E}^{t, x} \\Big[ \\int_t^T X_s^T C_s X_s + \\frac{1}{2}u_s^T D_s u_s ds + X_T^T R X_T\\Big],\n",
    "$$\n",
    "with $C(t) = C \\leq 0, R \\leq 0$, and $D=D(t) < -\\delta < 0$ given and deterministic ($\\delta > 0$ some constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-celtic",
   "metadata": {},
   "source": [
    "We write down the problem in its primal form as\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\partial_t J(t, x) + \\inf_{u} \\Big\\{ \\frac{1}{2} \\sigma^2 \\partial_{xx} J(t, x) + [H x + M u] \\partial_x J(t, x) + C x^2 + \\frac{1}{2}D u^2 \\Big\\} = 0 \\text{ on $[0, T] \\times (-\\infty, \\infty)$}\n",
    "\\\\\n",
    "J(T, x) = Rx^2 \\text{ $\\forall x \\in \\mathbb{R}$}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-ivory",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "    &\\begin{cases}\n",
    "        dX_t = H(t, X_t) dt + \\sigma(t, X_t) dW_t, \\quad t \\in [0, T] \\\\\n",
    "        X_0 = x\n",
    "    \\end{cases}, \\\\\n",
    "    &\\begin{cases}\n",
    "        dY_t = C(X_t) dt - \\frac{1}{2}(\\nabla J^{* \\; T} M D^{-1} M^T \\nabla J^*)(t, X_t) dt + \\nabla J^{* \\; T}(t, X_t) \\sigma(t, X_t)  dW_t, \\quad t \\in [0, T] \\\\\n",
    "        Y_T = g(X_T)\n",
    "    \\end{cases}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "LQR_MODEL_CONFIG = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_discretisation_steps\": 30,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"network_type\": \"MINI\",\n",
    "    \"optimiser\": \"Adam\"\n",
    "}\n",
    "model = LQR_MODEL_CONFIG\n",
    "class LQR(FBSDE):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        sigma = 0.3*torch.ones((model[\"batch_size\"], 1, 1))\n",
    "        M = 2.0*torch.ones((model[\"batch_size\"], 1, 1))\n",
    "        C = lambda X: 2.0*X**2\n",
    "        inv_D = torch.inverse(torch.tensor([[0.2]]))\n",
    "        self.h = lambda X, Y, Z, t: C(X) + (1/2)*torch.einsum(\"bi, bij, bj -> b\", Z, M, torch.einsum(\"bij, bj -> bi\", (-inv_D @ M), Z)).unsqueeze(1)\n",
    "        \n",
    "        self.b = lambda X, t: 0.1*X\n",
    "        self.sigma = lambda X, t: sigma\n",
    "        \n",
    "        self.terminal_condition = lambda X: 0.001*X**2\n",
    "        \n",
    "        self.var_dim = 1\n",
    "        self.terminal_time = 1     \n",
    "        self.init_sampling_func = lambda X: (X-0.5)*2\n",
    "        self.control_noise = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq = LQR()\n",
    "solver = FBSDESolver(model, eq)\n",
    "summary(solver.Y_net, (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(list(solver.train(400)))\n",
    "plot_losses(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'surface'}]])\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "ts = np.linspace(0, 1, 100)\n",
    "us_pred = np.array([[solver.J(x, t).item() for x in xs] for t in ts])\n",
    "fig.add_trace(go.Surface(x=xs, y=ts, z=us_pred), row=1, col=1)\n",
    "fig.update_layout(title='Solution | Approximation',\n",
    "                  scene = dict(\n",
    "                    xaxis_title=\"x\",\n",
    "                    yaxis_title=\"t\",\n",
    "                    zaxis_title=\"J(x, t\"),\n",
    "                  scene2 = dict(\n",
    "                    xaxis_title=\"x\",\n",
    "                    yaxis_title=\"t\",\n",
    "                    zaxis_title=\"J(x, t)\"),\n",
    "                  margin=dict(l=50, r=50, b=50, t=50))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=2.0\n",
    "sigma= 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = model[\"num_discretisation_steps\"]\n",
    "c_xs = np.zeros(n)\n",
    "c_xs[0] = -0.5\n",
    "uc_xs = np.zeros(n)\n",
    "uc_xs[0] = c_xs[0]\n",
    "dt = 1/n\n",
    "ts = [t for t in np.linspace(0, 1, n)]\n",
    "c_cum_cost = np.zeros(n)\n",
    "uc_cum_cost = np.zeros(n)\n",
    "for i in range(n-1):\n",
    "    dW = np.sqrt(dt)*np.random.randn()\n",
    "    c = solver.u(c_xs[i], i*dt).item()\n",
    "    uc = 0\n",
    "    c_xs[i+1] = c_xs[i] + (eq.H(c_xs[i], i*dt) + M*c)*dt + sigma*dW\n",
    "    uc_xs[i+1] = uc_xs[i] + (eq.H(uc_xs[i], i*dt) + M*uc)*dt + sigma*dW\n",
    "    c_cum_cost[i+1] = c_cum_cost[i] + eq.C(c_xs[i]) + eq.D*c**2\n",
    "    uc_cum_cost[i+1] = uc_cum_cost[i] + eq.C(uc_xs[i]) + eq.D*uc**2\n",
    "\n",
    "c_cum_cost[-1] += eq.terminal_condition(c_xs[-1])\n",
    "uc_cum_cost[-1] += eq.terminal_condition(uc_xs[-1])\n",
    "    \n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "fig.add_trace(go.Scatter(x=ts, y=c_xs, mode='lines', name=\"Controlled\", line=dict(color=\"#00e476\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ts, y=uc_xs, mode='lines', name=\"Uncontrolled\", line=dict(color=\"#FFe476\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ts, y=c_cum_cost, mode='lines', showlegend=False, line=dict(color=\"#00e476\")), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=ts, y=uc_cum_cost, mode='lines', showlegend=False, line=dict(color=\"#FFe476\")), row=1, col=2)\n",
    "fig.update_layout(\n",
    "    title=\"Minimise amount of X | Minimise the costs (hold both close to zero)\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"X\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=14\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-rochester",
   "metadata": {},
   "source": [
    "### Black-Scholes-Barenblatt Equation N-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSB_MODEL_CONFIG = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_discretisation_steps\": 30,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"network_type\": \"MINI\",\n",
    "    \"optimiser\": \"Adam\"\n",
    "}\n",
    "model = BSB_MODEL_CONFIG\n",
    "class BSB(FBSDE):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        r = 0.05\n",
    "        self.h = lambda X, Y, Z, t: r*(Y-torch.einsum(\"bi, bi -> b\", Z, X).unsqueeze(1))\n",
    "        \n",
    "        self.b = lambda X, t: 0.0*X\n",
    "        self.sigma = lambda X, t: 0.3*torch.diag_embed(X)\n",
    "        \n",
    "        self.terminal_condition = lambda X: torch.einsum(\"bi, bi-> b\", X, X).unsqueeze(1)\n",
    "        \n",
    "        self.var_dim = 2\n",
    "        self.terminal_time = 1     \n",
    "        self.init_sampling_func = lambda X: (X-0.5)*2\n",
    "        self.control_noise = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq = BSB()\n",
    "solver = FBSDESolver(model, eq)\n",
    "loss = np.array(list(solver.train(600)))\n",
    "plot_losses(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2\n",
    "def J_sol(X, t):\n",
    "    r = 0.05\n",
    "    sigma = 0.3\n",
    "    return np.exp((r + sigma**2)*(1 - t))*np.sum(X**2, axis=-1, keepdims=True)\n",
    "Xs, Y_preds, ts = solver.simulate_processes(num_samples)\n",
    "Y_sol = J_sol(Xs, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.add_trace(go.Scatter(x=ts[:, 0].flatten(), y=Y_preds[:, 0].flatten(), mode='lines', name=\"Prediction\", line=dict(color=\"#FFe476\")), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=ts[:, 0].flatten(), y=Y_sol[:, 0].flatten(), mode='lines', name=\"Ground truth\", line=dict(color=\"#00e476\")), row=1, col=1)\n",
    "for i in range(1, num_samples):\n",
    "    fig.add_trace(go.Scatter(x=ts[:, i].flatten(), y=Y_preds[:, i].flatten(), mode='lines', showlegend=False, line=dict(color=\"#FFe476\")), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=ts[:, i].flatten(), y=Y_sol[:, i].flatten(), mode='lines', showlegend=False, line=dict(color=\"#00e476\")), row=1, col=1)\n",
    "fig.update_layout(\n",
    "    title=\"Loss\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"J\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=14\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-patch",
   "metadata": {},
   "source": [
    "### Allen-Cahn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "AC_MODEL_CONFIG = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_discretisation_steps\": 15,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"learning_rate\": 5e-3,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"network_type\": \"MINI\",\n",
    "    \"optimiser\": \"Adam\"\n",
    "}\n",
    "model = AC_MODEL_CONFIG\n",
    "class AC(FBSDE):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.h = lambda X, Y, Z, t: Y - Y**3\n",
    "        \n",
    "        self.b = lambda X, t: 0.0*X\n",
    "        self.sigma = lambda X, t: torch.diag_embed(X*0+1)\n",
    "        \n",
    "        self.terminal_condition = lambda X: 1/(2+0.4*torch.sum(X**2, dim=-1, keepdims=True))\n",
    "        \n",
    "        self.var_dim = 20\n",
    "        self.terminal_time = 0.3   \n",
    "        self.init_sampling_func = lambda X: X*0\n",
    "        self.control_noise = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq = AC()\n",
    "solver = FBSDESolver(model, eq)\n",
    "loss = np.array(list(solver.train(500)))\n",
    "plot_losses(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=5\n",
    "Xs, Y_preds, ts = solver.simulate_processes(num_samples)\n",
    "fig = make_subplots(rows=1, cols=1)\n",
    "fig.add_trace(go.Scatter(x=ts[:, 0].flatten(), y=Y_preds[:, 0].flatten(), mode='lines', name=\"Prediction\", line=dict(color=\"#FFe476\")), row=1, col=1)\n",
    "for i in range(1, num_samples):\n",
    "    fig.add_trace(go.Scatter(x=ts[:, i].flatten(), y=Y_preds[:, i].flatten(), mode='lines', showlegend=False, line=dict(color=\"#FFe476\")), row=1, col=1)\n",
    "fig.update_layout(\n",
    "    title=\"Allen-Cahn\",\n",
    "    xaxis_title=\"t\",\n",
    "    yaxis_title=\"J\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=14\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.J(*([0]*3), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-badge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
